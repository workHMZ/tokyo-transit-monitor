import json
from datetime import datetime, timezone, timedelta
import time
import requests
from bs4 import BeautifulSoup

# --- è¨­å®š ---
TARGET_URL = 'https://transit.yahoo.co.jp/diainfo/area/4'
BASE_URL = 'https://transit.yahoo.co.jp'

# æ±äº¬éƒ½å†…å¸¸ç”¨è·¯ç·šã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆ
TOKYO_LINES = {
    # JRæ±æ—¥æœ¬ï¼ˆæ±äº¬éƒ½å†…å¸¸ç”¨è·¯ç·šï¼‰
    "å±±æ‰‹ç·š",
    "ä¸­å¤®ç·æ­¦ç·š(å„åœ)",
    "ä¸­å¤®ç·š(å¿«é€Ÿ)[æ±äº¬ï½é«˜å°¾]",
    "äº¬æµœæ±åŒ—æ ¹å²¸ç·š",
    "åŸ¼äº¬å·è¶Šç·š[ç¾½æ²¢æ¨ªæµœå›½å¤§ï½å·è¶Š]",
    "æ¹˜å—æ–°å®¿ãƒ©ã‚¤ãƒ³",
    "ä¸Šé‡æ±äº¬ãƒ©ã‚¤ãƒ³",
    "ç·æ­¦ç·š(å¿«é€Ÿ)[æ±äº¬ï½åƒè‘‰]",
    "äº¬è‘‰ç·š",
    "æ­¦è”µé‡ç·š",
    "å¸¸ç£ç·š(å¿«é€Ÿ)[å“å·ï½å–æ‰‹]",
    "å¸¸ç£ç·š(å„åœ)",
    "å—æ­¦ç·š[å·å´ï½ç«‹å·]",
    "æ¨ªé ˆè³€ç·š",
    
    # æ±äº¬ãƒ¡ãƒˆãƒ­
    "æ±äº¬ãƒ¡ãƒˆãƒ­éŠ€åº§ç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­ä¸¸ãƒå†…ç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­æ—¥æ¯”è°·ç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­æ±è¥¿ç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­åƒä»£ç”°ç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­æœ‰æ¥½ç”ºç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­åŠè”µé–€ç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­å—åŒ—ç·š",
    "æ±äº¬ãƒ¡ãƒˆãƒ­å‰¯éƒ½å¿ƒç·š",
    
    # éƒ½å–¶åœ°ä¸‹é‰„
    "éƒ½å–¶æµ…è‰ç·š",
    "éƒ½å–¶ä¸‰ç”°ç·š",
    "éƒ½å–¶æ–°å®¿ç·š",
    "éƒ½å–¶å¤§æ±Ÿæˆ¸ç·š",
    
    # äº¬ç‹é›»é‰„
    "äº¬ç‹ç·š",
    "äº¬ç‹æ–°ç·š",
    "äº¬ç‹ç›¸æ¨¡åŸç·š",
    "äº¬ç‹é«˜å°¾ç·š",
    "äº¬ç‹äº•ã®é ­ç·š",
    
    # å°ç”°æ€¥é›»é‰„
    "å°ç”°æ€¥å°ç”°åŸç·š",
    "å°ç”°æ€¥æ±Ÿãƒå³¶ç·š",
    "å°ç”°æ€¥å¤šæ‘©ç·š",
    
    # æ±æ€¥é›»é‰„
    "æ±æ€¥æ±æ¨ªç·š",
    "æ±æ€¥ç›®é»’ç·š",
    "æ±æ€¥ç”°åœ’éƒ½å¸‚ç·š",
    "æ±æ€¥å¤§äº•ç”ºç·š",
    "æ±æ€¥å¤šæ‘©å·ç·š",
    "æ±æ€¥æ± ä¸Šç·š",
    "æ±æ€¥ä¸–ç”°è°·ç·š",
    
    # è¥¿æ­¦é‰„é“
    "è¥¿æ­¦æ± è¢‹ç·šãƒ»ç§©çˆ¶ç·š",
    "è¥¿æ­¦æ–°å®¿ç·š",
    "è¥¿æ­¦å›½åˆ†å¯ºç·š",
    "è¥¿æ­¦å¤šæ‘©æ¹–ç·š",
    "è¥¿æ­¦æœ‰æ¥½ç”ºç·š",
    "è¥¿æ­¦æ‹å³¶ç·š",
    
    # ãã®ä»–æ±äº¬éƒ½å†…ä¾¿åˆ©ãªè·¯ç·š
    "æ—¥æš®é‡Œãƒ»èˆäººãƒ©ã‚¤ãƒŠãƒ¼",
    "ã‚†ã‚Šã‹ã‚‚ã‚ç·š",
    "æ±äº¬ãƒ¢ãƒãƒ¬ãƒ¼ãƒ«ç·š",
    "å¤šæ‘©éƒ½å¸‚ãƒ¢ãƒãƒ¬ãƒ¼ãƒ«ç·š",
}

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
}


def get_detail_page_info(session, detail_url, line_name, max_retries=3):
    """
    è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å®Œå…¨ãªé‹è¡Œæƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚
    
    Returns:
        dict: è·¯ç·šæƒ…å ± or Noneï¼ˆå–å¾—å¤±æ•—æ™‚ï¼‰
    """
    for attempt in range(max_retries):
        try:
            response = session.get(detail_url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸å†…ã®ä¸»è¦ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            detail_text = ""
            
            # é‹è¡Œæƒ…å ±ã®ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
            contents_body = soup.find(id='contents-body')
            if contents_body:
                # pã‚¿ã‚°ã‹ã‚‰è©³ç´°ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                for p in contents_body.find_all('p'):
                    text = p.get_text(strip=True)
                    # æ„å‘³ã®ã‚ã‚‹é•·ã•ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã€è·¯ç·šç™»éŒ²ãªã©ã®UIãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
                    if len(text) > 20 and 'è·¯ç·šã‚’ç™»éŒ²' not in text and 'è¿‚å›ãƒ«ãƒ¼ãƒˆ' not in text:
                        detail_text = text
                        break
            
            # çŠ¶æ…‹ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
            status = "é‹è»¢çŠ¶æ³"
            status_elem = soup.find(class_='labelStatus')
            if status_elem:
                status = status_elem.get_text(strip=True) or status
            
            if detail_text:
                return {
                    "line": line_name,
                    "status": status,
                    "detail": detail_text,
                    "url": detail_url
                }
            else:
                print(f"  âš ï¸ è©³ç´°ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {line_name}")
                return None
                
        except requests.exceptions.Timeout:
            print(f"  âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({attempt + 1}/{max_retries}): {line_name}")
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
            return None
        except requests.exceptions.RequestException as e:
            print(f"  âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {line_name} - {e}")
            return None
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {line_name} - {e}")
            return None
    
    return None


def scrape_transit_data():
    """
    äº¤é€šæƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€æ±äº¬éƒ½å†…ã®è·¯ç·šæƒ…å ±ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    print("äº¤é€šæƒ…å ±ã‚’å–å¾—ä¸­...")
    scraped_data = []
    
    try:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦æ¥ç¶šã‚’å†åˆ©ç”¨
        session = requests.Session()
        
        print(f"URL: {TARGET_URL} ã‹ã‚‰æœ€æ–°ã®é‹è¡Œæƒ…å ±ã‚’å–å¾—ä¸­...")
        response = session.get(TARGET_URL, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        print("âœ… ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—å®Œäº†ã€‚ãƒ‡ãƒ¼ã‚¿è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        
        # ç¾åœ¨é‹è¡Œæƒ…å ±ã®ã‚ã‚‹è·¯ç·šã®ãƒªãƒ³ã‚¯ã‚’åé›†
        troubled_line_links = {}
        
        # elmTblLstLine ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        table = soup.find(class_='elmTblLstLine')
        if table:
            for link in table.find_all('a'):
                line_name = link.get_text(strip=True)
                href = link.get('href')
                
                if line_name and href and line_name in TOKYO_LINES:
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if href.startswith('/'):
                        href = BASE_URL + href
                    troubled_line_links[line_name] = href
        
        if not troubled_line_links:
            print("âœ… æ±äº¬éƒ½å†…ã®è·¯ç·šã§é‹è¡Œæƒ…å ±ã®ã‚ã‚‹è·¯ç·šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return []
        
        print(f"\nğŸ“‹ é‹è¡Œæƒ…å ±ã®ã‚ã‚‹æ±äº¬éƒ½å†…è·¯ç·š: {len(troubled_line_links)}ä»¶")
        for name in troubled_line_links:
            print(f"  - {name}")
        
        # å„è·¯ç·šã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å®Œå…¨ãªæƒ…å ±ã‚’å–å¾—
        print("\nğŸ“– è©³ç´°æƒ…å ±ã‚’å–å¾—ä¸­...")
        for line_name, detail_url in troubled_line_links.items():
            print(f"  å–å¾—ä¸­: {line_name}")
            result = get_detail_page_info(session, detail_url, line_name)
            if result:
                scraped_data.append(result)
                print(f"  âœ… å–å¾—å®Œäº†: {line_name}")
            else:
                # è©³ç´°å–å¾—ã«å¤±æ•—ã—ã¦ã‚‚ã€åŸºæœ¬æƒ…å ±ã¯è¨˜éŒ²
                scraped_data.append({
                    "line": line_name,
                    "status": "é‹è»¢çŠ¶æ³",
                    "detail": "è©³ç´°æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                    "url": detail_url
                })
            
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            time.sleep(0.3)
        
        return scraped_data

    except requests.exceptions.RequestException as e:
        print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None
    except Exception as e:
        print(f"å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None


if __name__ == '__main__':
    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    all_lines_data = scrape_transit_data()
    
    # JSONå‡ºåŠ›å‡¦ç†
    if all_lines_data is not None:
        # æœ€çµ‚çš„ãªJSONæ§‹é€ ã‚’ä½œæˆ (JSTæ™‚é–“ã‚’ä½¿ç”¨)
        JST = timezone(timedelta(hours=9))
        output_json = {
            "update_time": datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S'),
            "data_source": TARGET_URL,
            "monitored_lines_count": len(TOKYO_LINES),
            "issue_count": len(all_lines_data),
            "status": "issues_found" if all_lines_data else "all_clear",
            "issues": all_lines_data
        }
        
        # JSONã‚’æ•´å½¢ã—ã¦ãƒ—ãƒªãƒ³ãƒˆ
        print("\n--- JSON Output ---")
        print(json.dumps(output_json, ensure_ascii=False, indent=4))
        
    else:
        print("\nâŒ é‹è¡Œæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")